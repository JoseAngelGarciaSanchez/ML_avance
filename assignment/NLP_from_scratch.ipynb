{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning avancé : Deep Learning for NLP\n",
    "On va mettre au point nos premiers modèles Deep Learning appliqué au traitement du langage. Plutôt que de tout recoder de zéro, on va ici s'appuyer sur la puissance de la librairie Keras qui nous permettra de connecter les layers à la volée et d'implémenter des architectures plus exotiques.\n",
    "\n",
    "Après cela vous saurez:\n",
    "- utiliser un embedding pré-calculé\n",
    "- construire un réseau de neurones avec Keras\n",
    "- construire une architecture custom avec Keras\n",
    "\n",
    "Cet exercice constitue également le test noté du cours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_utils.utils as du\n",
    "import data_utils.pos as pos \n",
    "import numpy as np\n",
    "import pandas\n",
    "data='data/'\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Embedding, Flatten, Input, Masking\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Présentation du problème\n",
    "Le POS-Tagging et le Shallow Parsing constituent deux tâches classiques en NLP :\n",
    "- POS-Tagging : affecte à chaque mot un tag unique qui indique son rôle syntaxique (nom, verbe, adverbe, ..)\n",
    "- Shallow Parsing : affecte à chaque segment de phrase un tag unique qui indique le rôle de l'élément syntaxique auquel il appartient (groupe nominal, groupe verbal, etc..)\n",
    "Les fichiers sont ici au format ConLL : un mot par ligne, les phrases sont séparées par un saut de ligne.\n",
    "\n",
    "Nous allons ici refaire les travaux de l'article NLP almost from scratch, qui consiste à créer un réseau de neurones pour effectuer chaque tâche, puis nous ferons un modèle partagé et enfin un modèle hiearchique.\n",
    "\n",
    "<img src=\"MLP.png\" style=\"width:300px;height:450px;\">\n",
    "<caption><center> <u>Figure 1</u>: Modèle simple pour les tâches POS-Tagging et Shallow Parsing</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS-Tagging\n",
    "## Import des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A l'aide des fonctions d'aide, importer les données :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordvector shape is (100003, 50)\n"
     ]
    }
   ],
   "source": [
    "wv, word_to_num, num_to_word = pos.load_wv(\n",
    "      data+'vocab.txt', data+'wordVectors.txt')\n",
    "print('wordvector shape is',wv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère également les tags et on crée les dictionnaires adéquats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création des tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagnames = ['ADJ','ADJWH','ADV','ADVWH','CC','CLO',\n",
    "                'CLR','CLS','CS','DET','DETWH','ET','I','NC',\n",
    "                'NPP','P','P+D','P+PRO','PONCT','PREF','PRO',\n",
    "                'PROREL','PROWH','VINF','VPR','VPP','V','VS','VIMP']\n",
    "num_to_tag = dict(enumerate(tagnames))\n",
    "tag_to_num = {v:k for k,v in num_to_tag.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chargement des données text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement des documents puis création des matrices X_train, y_train, X_test, y_test\n",
    "le paramètre wsize précise la taille de la fenêtre, choisissez une valeur par défaut parmis (3,5,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train = du.load_dataset(data+'train.txt') # liste qui contient les phrases et les tags\n",
    "X_train, y_train = du.docs_to_windows(docs_train, word_to_num, tag_to_num, wsize=3) # parcourt la liste et créer les matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['paul', 'NC'],\n",
       " ['jules', 'ADJ'],\n",
       " ['antoine', 'NC'],\n",
       " ['meillet', 'ADJ'],\n",
       " [',', 'PONCT'],\n",
       " ['né', 'VPP'],\n",
       " ['le', 'DET'],\n",
       " ['à', 'P'],\n",
       " ['moulins', 'NC'],\n",
       " ['(', 'PONCT'],\n",
       " ['allier', 'NC'],\n",
       " [')', 'PONCT'],\n",
       " ['et', 'CC'],\n",
       " ['mort', 'ADV'],\n",
       " ['le', '_ADV'],\n",
       " ['à', '_ADV'],\n",
       " ['châteaumeillant', '_ADV'],\n",
       " ['(', 'PONCT'],\n",
       " ['cher', 'ADJ'],\n",
       " [')', 'PONCT'],\n",
       " [',', 'PONCT'],\n",
       " ['est', 'V'],\n",
       " ['le', 'DET'],\n",
       " ['principal', 'ADJ'],\n",
       " ['linguiste', 'NC'],\n",
       " ['français', 'ADJ'],\n",
       " ['des', 'P+D'],\n",
       " ['premières', 'ADJ'],\n",
       " ['décennies', 'NC'],\n",
       " ['du', 'P+D'],\n",
       " ['.', 'PONCT']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_test = du.load_dataset(data+'test.txt')\n",
    "X_test, y_test = du.docs_to_windows(\n",
    "    docs_test, word_to_num, tag_to_num, wsize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, 29)\n",
    "y_test = keras.utils.to_categorical(y_test, 29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On extrait les lignes dans y_test qui contiennent des mots non-présents dans X_train : out of vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_oov,Y_test_oov = du.get_oov(X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train a pour dimension (680238, 3)\n",
      "X_test a pour dimension (267334, 3)\n",
      "y_train a pour dimension (680238, 29)\n",
      "y_test a pour dimension (267334, 29)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train a pour dimension\",X_train.shape)\n",
    "print(\"X_test a pour dimension\",X_test.shape)\n",
    "print(\"y_train a pour dimension\",y_train.shape)\n",
    "print(\"y_test a pour dimension\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question :\n",
    "\n",
    "Expliquer ce que contient X et Y ci-dessus ainsi que leur dimension. (NB: Lorsque l'on définit une fenêtre on est amené à créer un tag pour le début et la fin de phrase afin que les premiers et derniers mots puissent être considérés dans le modèle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réponse :\n",
    "\n",
    "X_train et X_test contiennent les fenêtres de mots qui seront utilisées comme entrées pour le modèle. Chaque fenêtre est constituée de trois mots, où le mot central est l'étiquette à prédire et les deux mots voisins sont les contextes. Les dimensions de X_train et X_test signifie qu'il y a 680238 et 267334 fenêtres dans les ensembles d'entraînement et de test, respectivement, chacune avec trois mots.\n",
    "\n",
    "y_train et y_test contiennent les étiquettes. Les dimensions de y_train et y_test signifie qu'il y a 680238 et 267334 fenêtres dans les ensembles d'entraînement et de test, respectivement, chacune avec une étiquette correspondant à l'une des 29 classes de POS tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consigne :\n",
    "Compléter le code ci-dessous pour définir une architecture :\n",
    "Embed (dim 50) -> Dense -> Dropout -> Predict (Softmax).\n",
    "N'hésitez pas à consulter l'aide de Keras pour :\n",
    "    - models.Sequential\n",
    "    - layers.Embedding\n",
    "    - layers.Flatten\n",
    "NB : Prendre garde à bien laisser l'embedding \"entrainable\" afin que la représentation vectorielle bénéficie aussi de la backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# Your code Here#\n",
    "model.add(Embedding(input_dim=len(word_to_num), output_dim=50, input_length=3, trainable=True))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(29, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions :\n",
    "- A quoi servent les layers suivants :\n",
    "    - Flatten()\n",
    "    - Dropout()\n",
    "- Combien de paramètres seront appris au cours de l'entrainement? (il existe une commande qui permette de trouver l'architecture du réseau de neurones)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réponse\n",
    "Flatten() est utilisé pour convertir la sortie du précédent Dense layer en un vecteur 1D pour qu'elle puisse être utilisée par la couche suivante.\n",
    "\n",
    "Dropout() est une méthode de régularisation pour prévenir le overfitting en réduisant les effets de la coadaptation des neurones. Elle éteint aléatoirement une proportion des neurones d'entrée à chaque étape de l'entraînement.\n",
    "\n",
    "Le nombre de paramètres est de 5000150 pour l'embedding, 3264 pour la couche dense, et 11165 pour la dernière couche dense total = 5.014.579 parametres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consigne : Compilation\n",
    "Compléter le code ci-dessous en choisissant l'optimizer RMSprop, et en choissisant la bonne loss (NB on est sur un probléme de classification à 29 modalités). La métrique sera renvoyée par les logs au cours de l'entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consigne  : Entrainement\n",
    "Compléter le code ci-dessous pour entrainer sur le couple X_train, Y_train et en validant sur X_test, Y_test. Vous devriez pouvoir atteindre une accuracy de 93% au bout de 3 itérations. N'oubliez pas que la taille du batch correspond au nombre d'exemples utilisés pour estimer le gradient. Il peut influer sur la convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "5315/5315 [==============================] - 76s 14ms/step - loss: 0.4986 - accuracy: 0.8657 - val_loss: 0.2969 - val_accuracy: 0.9163\n",
      "Epoch 2/3\n",
      "5315/5315 [==============================] - 75s 14ms/step - loss: 0.3135 - accuracy: 0.9150 - val_loss: 0.2710 - val_accuracy: 0.9234\n",
      "Epoch 3/3\n",
      "5315/5315 [==============================] - 92s 17ms/step - loss: 0.2933 - accuracy: 0.9210 - val_loss: 0.2647 - val_accuracy: 0.9258\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=3,\n",
    "                    batch_size=128,\n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question :\n",
    "- Nous avons décidé de regarder l'accuracy comme métrique. Expliquez en quoi ce choix est discutable.\n",
    "- On se propose de tester la performance sur des données qui contiennent des mots non-vus lors du train dans la cellule ci-dessous lancer l'évaluation sur X_test_oov,Y_test_oov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réponse :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elle peut être trompeuse, par exemple avec des classes déséquilibrées. On pourrait regarder la précision, le rappel ou le F1-score. Sinon pour les problèmes des imbalaced classes, la bacc (balance accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "779/779 [==============================] - 1s 1ms/step - loss: 0.4421 - accuracy: 0.8713\n",
      "0.871318519115448\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test_oov, Y_test_oov)\n",
    "print(score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consigne :\n",
    "- Déterminer quelle est la meilleur taille de fenêtre selon vous (tester les plusieurs hypothèses parmis 3, 5 et 7)\n",
    "- Reprendre le code pour les différentes hypothèses suivantes avec la taille de fenêtre choisie ci-dessus :\n",
    "    - 1 embedding random non entrainable\n",
    "    - 2 embedding pré-entrainer (wv) entrainable\n",
    "    - 3 embedding pré-entrainer (wv) non entrainable\n",
    "- Conclure en testant sur X_test_oov et Y_test_oov :\n",
    "    - Quelle est la méthode que vous choisiriez et pourquoi? \n",
    "    - Pouvez-vous expliquer intuitivement les différents résultats ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window: 3\n",
      "window size 3 accuracy,0.79913330078125\n",
      "Window: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-29 22:48:21.668553: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: Matrix size-incompatible: In[0]: [32,384], In[1]: [640,29]\n",
      "\t [[{{node sequential_13/dense_21/BiasAdd}}]]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'sequential_13/dense_21/BiasAdd' defined at (most recent call last):\n    File \"/Users/pepegarcia/opt/anaconda3/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/Users/pepegarcia/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/Users/pepegarcia/opt/anaconda3/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/Users/pepegarcia/opt/anaconda3/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/Users/pepegarcia/opt/anaconda3/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_cell\n      result = self._run_cell(\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3016, in _run_cell\n      result = runner(coro)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3221, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/vz/nf90bfdj7mg7_f82cw3921dh0000gn/T/ipykernel_47936/1704558911.py\", line 32, in <module>\n      score = model.evaluate(X_test_oov, Y_test_oov, verbose=0)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/keras/engine/training.py\", line 2072, in evaluate\n      tmp_logs = self.test_function(iterator)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/keras/engine/training.py\", line 1852, in test_function\n      return step_function(self, iterator)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/keras/engine/training.py\", line 1836, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/keras/engine/training.py\", line 1824, in run_step\n      outputs = model.test_step(data)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/keras/engine/training.py\", line 1788, in test_step\n      y_pred = self(x, training=False)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/keras/engine/sequential.py\", line 412, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/keras/engine/functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/keras/engine/functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/keras/layers/core/dense.py\", line 252, in call\n      outputs = tf.nn.bias_add(outputs, self.bias)\nNode: 'sequential_13/dense_21/BiasAdd'\nMatrix size-incompatible: In[0]: [32,384], In[1]: [640,29]\n\t [[{{node sequential_13/dense_21/BiasAdd}}]] [Op:__inference_test_function_834271]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 32\u001b[0m\n\u001b[1;32m     22\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     23\u001b[0m               optimizer\u001b[39m=\u001b[39mRMSprop(),\n\u001b[1;32m     24\u001b[0m               metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     26\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(X_train, y_train,\n\u001b[1;32m     27\u001b[0m                     epochs\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,\n\u001b[1;32m     28\u001b[0m                     batch_size\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m,\n\u001b[1;32m     29\u001b[0m                     validation_data\u001b[39m=\u001b[39m(X_test, y_test),\n\u001b[1;32m     30\u001b[0m                     verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m score \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mevaluate(X_test_oov, Y_test_oov, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     33\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mwindow size \u001b[39m\u001b[39m{\u001b[39;00mwsize\u001b[39m}\u001b[39;00m\u001b[39m accuracy,\u001b[39m\u001b[39m{\u001b[39;00mscore[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'sequential_13/dense_21/BiasAdd' defined at (most recent call last):\n    File \"/Users/pepegarcia/opt/anaconda3/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/Users/pepegarcia/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/Users/pepegarcia/opt/anaconda3/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/Users/pepegarcia/opt/anaconda3/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/Users/pepegarcia/opt/anaconda3/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_cell\n      result = self._run_cell(\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3016, in _run_cell\n      result = runner(coro)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3221, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/vz/nf90bfdj7mg7_f82cw3921dh0000gn/T/ipykernel_47936/1704558911.py\", line 32, in <module>\n      score = model.evaluate(X_test_oov, Y_test_oov, verbose=0)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/keras/engine/training.py\", line 2072, in evaluate\n      tmp_logs = self.test_function(iterator)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/keras/engine/training.py\", line 1852, in test_function\n      return step_function(self, iterator)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/keras/engine/training.py\", line 1836, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/keras/engine/training.py\", line 1824, in run_step\n      outputs = model.test_step(data)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/keras/engine/training.py\", line 1788, in test_step\n      y_pred = self(x, training=False)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/keras/engine/sequential.py\", line 412, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/keras/engine/functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/keras/engine/functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/pepegarcia/Documents/GitHub/ML_avance/venv/lib/python3.9/site-packages/keras/layers/core/dense.py\", line 252, in call\n      outputs = tf.nn.bias_add(outputs, self.bias)\nNode: 'sequential_13/dense_21/BiasAdd'\nMatrix size-incompatible: In[0]: [32,384], In[1]: [640,29]\n\t [[{{node sequential_13/dense_21/BiasAdd}}]] [Op:__inference_test_function_834271]"
     ]
    }
   ],
   "source": [
    "# Define window sizes to test\n",
    "window_sizes = [3, 5, 7]\n",
    "\n",
    "# Loop over window sizes\n",
    "for wsize in window_sizes:\n",
    "    \n",
    "    print(f'Window: {wsize}')\n",
    "    \n",
    "    X_train, y_train = du.docs_to_windows(docs_train, word_to_num, tag_to_num, wsize=wsize)\n",
    "    X_test, y_test = du.docs_to_windows(docs_test, word_to_num, tag_to_num, wsize=wsize)\n",
    "    \n",
    "    y_train = keras.utils.to_categorical(y_train, 29)\n",
    "    y_test = keras.utils.to_categorical(y_test, 29)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(word_to_num), output_dim=50, input_length=wsize, trainable=False))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(29, activation='softmax'))\n",
    "\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=RMSprop(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        epochs=3,\n",
    "                        batch_size=128,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        verbose=0)\n",
    "    \n",
    "    score = model.evaluate(X_test_oov, Y_test_oov, verbose=0)\n",
    "    print(f'window size {wsize} accuracy,{score[1]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est possible de sauvegarder le modèle (les poids W) dans un fichier .h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-29 22:38:44.748961: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,3,50]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-03-29 22:38:44.754927: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,3,64]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-03-29 22:38:44.967337: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,3,50]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-03-29 22:38:45.001210: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,3,64]\n",
      "\t [[{{node inputs}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"./model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shallow Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consigne :\n",
    "- Reprendre la méthodologie ci-dessus et entrainer un modèle de shallow parsing\n",
    "- Les tags sont ['O','B-NP','I-NP','B-AP','I-AP','B-CONJ',\n",
    "                'I-CONJ','B-AdP','I-AdP','B-VN','I-VN','B-PP','I-PP','B-UNKNOWN','I-UNKNOWN']\n",
    "- Les fichiers sont train_chunk.txt et test_chunk.txt\n",
    "- N'oubliez pas de recréer les tests out-of-vocabulary (X_test_oov et Y_test_oov)\n",
    "- Conclure sur votre choix du meilleur modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word vectors\n",
    "wv, word_to_num, num_to_word = pos.load_wv(data+'vocab.txt', data+'wordVectors.txt')\n",
    "\n",
    "# Load data\n",
    "docs_train = du.load_dataset(data+'train_chunk.txt')\n",
    "X_train, y_train = du.docs_to_windows(docs_train, word_to_num, tag_to_num, wsize=5)\n",
    "docs_test = du.load_dataset(data+'test_chunk.txt')\n",
    "X_test, y_test = du.docs_to_windows(docs_test, word_to_num, tag_to_num, wsize=5)\n",
    "\n",
    "# Convert labels to categorical format\n",
    "y_train = keras.utils.to_categorical(y_train, 15)\n",
    "y_test = keras.utils.to_categorical(y_test, 15)\n",
    "\n",
    "# Create out-of-vocabulary test set\n",
    "X_test_oov, Y_test_oov = du.get_oov(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Define model architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(word_to_num), output_dim=50, input_length=5, trainable=True))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(15, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=3,\n",
    "                    batch_size=128,\n",
    "                    validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate model on out-of-vocabulary test set\n",
    "score = model.evaluate(X_test_oov, Y_test_oov, verbose=0)\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# Save model weights to file\n",
    "model.save('shallow_parser.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création des tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création du réseau de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-task learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full multi tagged (POS tagging + Shallow parsing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En deep learning, il est possible et parfois même recommandé d'entrainer un réseau de neurones sur plusieurs tâches en même temps. Intuitivement on se dit que l'apprentissage de représentation sur une tâche devrait pouvoir aider sur une autre tâche. Cela permet en outre de disposer de plus de données par exemple et/ou d'avoir un modèle plus robuste, plus précis.\n",
    "\n",
    "Dans cette partie, on va s'appuyer sur la classe Model de Keras https://keras.io/getting-started/functional-api-guide/. Plutôt que d'ajouter les layers, il s'agit plutôt de voire chaque layer comme une fonction. Il s'agit alors d'enchainer les fonctions.\n",
    "\n",
    "<img src=\"mtl_images.png\" style=\"width:300px;height:450px;\">\n",
    "<caption><center> <u>Figure 1</u>: Un exemple d'architecture pour apprendre 3 tâches </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv, word_to_num, num_to_word = pos.load_wv(\n",
    "      data+'vocab.txt', data+'wordVectors.txt')\n",
    "print('wordvector shape is',wv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création des tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postagnames = ['ADJ','ADJWH','ADV','ADVWH','CC','CLO',\n",
    "                'CLR','CLS','CS','DET','DETWH','ET','I','NC',\n",
    "                'NPP','P','P+D','P+PRO','PONCT','PREF','PRO',\n",
    "                'PROREL','PROWH','VINF','VPR','VPP','V','VS','VIMP']\n",
    "num_to_postag = dict(enumerate(postagnames))\n",
    "postag_to_num = {v:k for k,v in num_to_postag.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunktagnames = ['O','B-NP','I-NP','B-AP','I-AP','B-CONJ',\n",
    "                'I-CONJ','B-AdP','I-AdP','B-VN','I-VN','B-PP','I-PP','B-UNKNOWN','I-UNKNOWN']\n",
    "num_to_chunktag = dict(enumerate(chunktagnames))\n",
    "chunktag_to_num = {v:k for k,v in num_to_chunktag.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training set\n",
    "docs = du.load_dataset(data+'train.txt')\n",
    "X_train, pos_train = du.docs_to_windows(\n",
    "    docs, word_to_num, postag_to_num, wsize=5)\n",
    "docs = du.load_dataset(data+'train_chunk')\n",
    "X_train, chunk_train = du.docs_to_windows(\n",
    "    docs, word_to_num, chunktag_to_num, wsize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = du.load_dataset(data+'test.txt')\n",
    "X_test, pos_test = du.docs_to_windows(\n",
    "    docs, word_to_num, postag_to_num, wsize=5)\n",
    "docs = du.load_dataset(data+'test_chunk')\n",
    "X_test, chunk_test = du.docs_to_windows(\n",
    "    docs, word_to_num, chunktag_to_num, wsize=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mise en forme pour l'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pos_train=X_train[:400000,:]\n",
    "\n",
    "X_chunk_train=X_train[-400000:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pos_train = keras.utils.to_categorical(pos_train[0:400000,], 29)\n",
    "Y_pos_test = keras.utils.to_categorical(pos_test, 29)\n",
    "Y_chunk_train = keras.utils.to_categorical(chunk_train, 15)\n",
    "Y_chunk_test = keras.utils.to_categorical(chunk_test[-400000:,], 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Résumé\n",
    "Jusqu'à présent on dispose de :\n",
    "- wv qui contient un embedding\n",
    "- X_pos_ et Y_pos_ qui contiennent respectivement les mots et le tag sur train et test pour le pos tagging\n",
    "- X_chunk_ et Y_chunk_ qui contiennent respectivement les mots et le tag sur train et test pour le shallow parsing\n",
    "- X_pos et X_chunk n'ont aucune fenêtre en commun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du réseaux de neurones commun aux deux tâches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consigne\n",
    "Utiliser la même architecture que précedemment pour construire le réseau de neurones communs : shared_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_nn=Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consigne\n",
    "Définir les inputs du réseaux de neurones communs à l'aide de la fonction Input. Ce sont les input layer de notre architecture, chacun correspond à une tâche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_input = \n",
    "chunk_input ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consigne\n",
    "- Définir pos_representation et chunk_representation comme les images respectibes de chaque input par shared_nn\n",
    "- Définir pos_target et chunk_target comme l'image des représentations ci-dessus par des layers dense avec activation softmax (cf  la fonction Dense)\n",
    "\n",
    "Rappel : la fonction softmax va calculer $n$ softmax, où $n$ est le nombre de classe en suivant la formule :\n",
    "$$\\sigma(z)_j = \\frac{\\exp(z_j)}{\\sum_{i}\\exp(z_i)},$$\n",
    "avec $z$ le vecteur en sortie du réseau de neurones et $z_i$ sa $i$-ième composante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_representation =\n",
    "chunk_representation =\n",
    "\n",
    "pos_target = \n",
    "chunk_target = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement et conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consigne \n",
    "Jusqu'ici on a juste défini la succession d'opération, il s'agit maintenant de créer le modèle en utilisant la classe modèle de Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Model(inputs=,outputs=)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consigne\n",
    "- compiler le modèle (NB : il y a une loss pour chaque tâche)\n",
    "- afficher l'architecture\n",
    "- entrainer le modèle pour le nombre d'époque suffisant en validant sur X_test et Y_test et une taille de batch de 128\n",
    "- Conclure en comparant avec les performances précédentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical learning\n",
    "Une autre façon de faire du multi-task consiste à construire une architecture en cascade où les tâches n'interviennent pas à la même profondeur du réseau de neurones.\n",
    "\n",
    "### Consigne :\n",
    "- En vous inspirant de la partie précedente, entrainer un modèle en cascade de type : \n",
    "                              POS\n",
    "                            /\n",
    "EMBEDDING - DENSE - DROPOUT \n",
    "                            \\\n",
    "                              DENSE - DROPOUT - CHUNK\n",
    "                              \n",
    "Autrement dit, dans cette approche, on pré-suppose que  \n",
    "- EMBEDDING - DENSE - DROPOUT apprend une représentation suffisante pour prédire le POS-TAG\n",
    "- plus de layer sont nécessaires pour le Shallow parsing\n",
    "- la représentation intermédiaire EMBEDDING - DENSE - DROPOUT est un bon point de départ pour le shallow parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question optionnelle \n",
    "- Comment feriez vous, en utilisant la classe Model, pour pouvoir extraire l'embedding tuné sur le modèle pour un mot donné? Tenter de l'implémenter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
