{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning avancé : Deep Learning for NLP\n",
    "On va mettre au point nos premiers modèles Deep Learning appliqué au traitement du langage. Plutôt que de tout recoder de zéro, on va ici s'appuyer sur la puissance de la librairie Keras qui nous permettra de connecter les layers à la volée et d'implémenter des architectures plus exotiques.\n",
    "\n",
    "Après cela vous saurez:\n",
    "- utiliser un embedding pré-calculé\n",
    "- construire un réseau de neurones avec Keras\n",
    "- construire une architecture custom avec Keras\n",
    "\n",
    "Cet exercice constitue également le test noté du cours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_utils.utils as du\n",
    "import data_utils.pos as pos \n",
    "import numpy as np\n",
    "import pandas\n",
    "data='data/'\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Embedding, Flatten, Input, Masking\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Présentation du problème\n",
    "Le POS-Tagging et le Shallow Parsing constituent deux tâches classiques en NLP :\n",
    "- POS-Tagging : affecte à chaque mot un tag unique qui indique son rôle syntaxique (nom, verbe, adverbe, ..)\n",
    "- Shallow Parsing : affecte à chaque segment de phrase un tag unique qui indique le rôle de l'élément syntaxique auquel il appartient (groupe nominal, groupe verbal, etc..)\n",
    "Les fichiers sont ici au format ConLL : un mot par ligne, les phrases sont séparées par un saut de ligne.\n",
    "\n",
    "Nous allons ici refaire les travaux de l'article NLP almost from scratch, qui consiste à créer un réseau de neurones pour effectuer chaque tâche, puis nous ferons un modèle partagé et enfin un modèle hiearchique.\n",
    "\n",
    "<img src=\"MLP.png\" style=\"width:300px;height:450px;\">\n",
    "<caption><center> <u>Figure 1</u>: Modèle simple pour les tâches POS-Tagging et Shallow Parsing</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS-Tagging\n",
    "## Import des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A l'aide des fonctions d'aide, importer les données :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wv, word_to_num, num_to_word = pos.load_wv(\n",
    "      data+'vocab.txt', data+'wordVectors.txt')\n",
    "print('wordvector shape is',wv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère également les tags et on crée les dictionnaires adéquats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création des tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagnames = ['ADJ','ADJWH','ADV','ADVWH','CC','CLO',\n",
    "                'CLR','CLS','CS','DET','DETWH','ET','I','NC',\n",
    "                'NPP','P','P+D','P+PRO','PONCT','PREF','PRO',\n",
    "                'PROREL','PROWH','VINF','VPR','VPP','V','VS','VIMP']\n",
    "num_to_tag = dict(enumerate(tagnames))\n",
    "tag_to_num = {v:k for k,v in num_to_tag.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chargement des données text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement des documents puis création des matrices X_train, y_train, X_test, y_test\n",
    "le paramètre wsize précise la taille de la fenêtre, choisissez une valeur par défaut parmis (3,5,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train = du.load_dataset(data+'train.txt') # liste qui contient les phrases et les tags\n",
    "X_train, y_train = du.docs_to_windows(docs_train, word_to_num, tag_to_num, wsize=) # parcourt la liste et créer les matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_test = du.load_dataset(data+'test.txt')\n",
    "X_test, y_test = du.docs_to_windows(\n",
    "    docs_test, word_to_num, tag_to_num, wsize=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, 29)\n",
    "y_test = keras.utils.to_categorical(y_test, 29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On extrait les lignes dans y_test qui contiennent des mots non-présents dans X_train : out of vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_oov,Y_test_oov = du.get_oov(X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train a pour dimension\",X_train.shape)\n",
    "print(\"X_test a pour dimension\",X_test.shape)\n",
    "print(\"y_train a pour dimension\",y_train.shape)\n",
    "print(\"y_test a pour dimension\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question :\n",
    "\n",
    "Expliquer ce que contient X et Y ci-dessus ainsi que leur dimension. (NB: Lorsque l'on définit une fenêtre on est amené à créer un tag pour le début et la fin de phrase afin que les premiers et derniers mots puissent être considérés dans le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réponse :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consigne :\n",
    "Compléter le code ci-dessous pour définir une architecture :\n",
    "Embed (dim 50) -> Dense -> Dropout -> Predict (Softmax).\n",
    "N'hésitez pas à consulter l'aide de Keras pour :\n",
    "    - models.Sequential\n",
    "    - layers.Embedding\n",
    "    - layers.Flatten\n",
    "NB : Prendre garde à bien laisser l'embedding \"entrainable\" afin que la représentation vectorielle bénéficie aussi de la backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# Your code Here#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions :\n",
    "- A quoi servent les layers suivants :\n",
    "    - Flatten()\n",
    "    - Dropout()\n",
    "- Combien de paramètres seront appris au cours de l'entrainement? (il existe une commande qui permette de trouver l'architecture du réseau de neurones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consigne : Compilation\n",
    "Compléter le code ci-dessous en choisissant l'optimizer RMSprop, et en choissisant la bonne loss (NB on est sur un probléme de classification à 29 modalités). La métrique sera renvoyée par les logs au cours de l'entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = ,\n",
    "              optimizer = ,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consigne  : Entrainement\n",
    "Compléter le code ci-dessous pour entrainer sur le couple X_train, Y_train et en validant sur X_test, Y_test. Vous devriez pouvoir atteindre une accuracy de 93% au bout de 3 itérations. N'oubliez pas que la taille du batch correspond au nombre d'exemples utilisés pour estimer le gradient. Il peut influer sur la convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question :\n",
    "- Nous avons décidé de regarder l'accuracy comme métrique. Expliquez en quoi ce choix est discutable.\n",
    "- On se propose de tester la performance sur des données qui contiennent des mots non-vus lors du train dans la cellule ci-dessous lancer l'évaluation sur X_test_oov,Y_test_oov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réponse :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consigne :\n",
    "- Déterminer quelle est la meilleur taille de fenêtre selon vous (tester les plusieurs hypothèses parmis 3, 5 et 7)\n",
    "- Reprendre le code pour les différentes hypothèses suivantes avec la taille de fenêtre choisie ci-dessus :\n",
    "    - 1 embedding random non entrainable\n",
    "    - 2 embedding pré-entrainer (wv) entrainable\n",
    "    - 3 embedding pré-entrainer (wv) non entrainable\n",
    "- Conclure en testant sur X_test_oov et Y_test_oov :\n",
    "    - Quelle est la méthode que vous choisiriez et pourquoi? \n",
    "    - Pouvez-vous expliquer intuitivement les différents résultats ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est possible de sauvegarder le modèle (les poids W) dans un fichier .h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shallow Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consigne :\n",
    "- Reprendre la méthodologie ci-dessus et entrainer un modèle de shallow parsing\n",
    "- Les tags sont ['O','B-NP','I-NP','B-AP','I-AP','B-CONJ',\n",
    "                'I-CONJ','B-AdP','I-AdP','B-VN','I-VN','B-PP','I-PP','B-UNKNOWN','I-UNKNOWN']\n",
    "- Les fichiers sont train_chunk.txt et test_chunk.txt\n",
    "- N'oubliez pas de recréer les tests out-of-vocabulary (X_test_oov et Y_test_oov)\n",
    "- Conclure sur votre choix du meilleur modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création des tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création du réseau de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-task learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full multi tagged (POS tagging + Shallow parsing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En deep learning, il est possible et parfois même recommandé d'entrainer un réseau de neurones sur plusieurs tâches en même temps. Intuitivement on se dit que l'apprentissage de représentation sur une tâche devrait pouvoir aider sur une autre tâche. Cela permet en outre de disposer de plus de données par exemple et/ou d'avoir un modèle plus robuste, plus précis.\n",
    "\n",
    "Dans cette partie, on va s'appuyer sur la classe Model de Keras https://keras.io/getting-started/functional-api-guide/. Plutôt que d'ajouter les layers, il s'agit plutôt de voire chaque layer comme une fonction. Il s'agit alors d'enchainer les fonctions.\n",
    "\n",
    "<img src=\"mtl_images.png\" style=\"width:300px;height:450px;\">\n",
    "<caption><center> <u>Figure 1</u>: Un exemple d'architecture pour apprendre 3 tâches </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv, word_to_num, num_to_word = pos.load_wv(\n",
    "      data+'vocab.txt', data+'wordVectors.txt')\n",
    "print('wordvector shape is',wv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création des tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postagnames = ['ADJ','ADJWH','ADV','ADVWH','CC','CLO',\n",
    "                'CLR','CLS','CS','DET','DETWH','ET','I','NC',\n",
    "                'NPP','P','P+D','P+PRO','PONCT','PREF','PRO',\n",
    "                'PROREL','PROWH','VINF','VPR','VPP','V','VS','VIMP']\n",
    "num_to_postag = dict(enumerate(postagnames))\n",
    "postag_to_num = {v:k for k,v in num_to_postag.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunktagnames = ['O','B-NP','I-NP','B-AP','I-AP','B-CONJ',\n",
    "                'I-CONJ','B-AdP','I-AdP','B-VN','I-VN','B-PP','I-PP','B-UNKNOWN','I-UNKNOWN']\n",
    "num_to_chunktag = dict(enumerate(chunktagnames))\n",
    "chunktag_to_num = {v:k for k,v in num_to_chunktag.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training set\n",
    "docs = du.load_dataset(data+'train.txt')\n",
    "X_train, pos_train = du.docs_to_windows(\n",
    "    docs, word_to_num, postag_to_num, wsize=5)\n",
    "docs = du.load_dataset(data+'train_chunk')\n",
    "X_train, chunk_train = du.docs_to_windows(\n",
    "    docs, word_to_num, chunktag_to_num, wsize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = du.load_dataset(data+'test.txt')\n",
    "X_test, pos_test = du.docs_to_windows(\n",
    "    docs, word_to_num, postag_to_num, wsize=5)\n",
    "docs = du.load_dataset(data+'test_chunk')\n",
    "X_test, chunk_test = du.docs_to_windows(\n",
    "    docs, word_to_num, chunktag_to_num, wsize=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mise en forme pour l'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pos_train=X_train[:400000,:]\n",
    "\n",
    "X_chunk_train=X_train[-400000:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pos_train = keras.utils.to_categorical(pos_train[0:400000,], 29)\n",
    "Y_pos_test = keras.utils.to_categorical(pos_test, 29)\n",
    "Y_chunk_train = keras.utils.to_categorical(chunk_train, 15)\n",
    "Y_chunk_test = keras.utils.to_categorical(chunk_test[-400000:,], 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Résumé\n",
    "Jusqu'à présent on dispose de :\n",
    "- wv qui contient un embedding\n",
    "- X_pos_ et Y_pos_ qui contiennent respectivement les mots et le tag sur train et test pour le pos tagging\n",
    "- X_chunk_ et Y_chunk_ qui contiennent respectivement les mots et le tag sur train et test pour le shallow parsing\n",
    "- X_pos et X_chunk n'ont aucune fenêtre en commun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du réseaux de neurones commun aux deux tâches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consigne\n",
    "Utiliser la même architecture que précedemment pour construire le réseau de neurones communs : shared_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_nn=Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consigne\n",
    "Définir les inputs du réseaux de neurones communs à l'aide de la fonction Input. Ce sont les input layer de notre architecture, chacun correspond à une tâche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_input = \n",
    "chunk_input ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consigne\n",
    "- Définir pos_representation et chunk_representation comme les images respectibes de chaque input par shared_nn\n",
    "- Définir pos_target et chunk_target comme l'image des représentations ci-dessus par des layers dense avec activation softmax (cf  la fonction Dense)\n",
    "\n",
    "Rappel : la fonction softmax va calculer $n$ softmax, où $n$ est le nombre de classe en suivant la formule :\n",
    "$$\\sigma(z)_j = \\frac{\\exp(z_j)}{\\sum_{i}\\exp(z_i)},$$\n",
    "avec $z$ le vecteur en sortie du réseau de neurones et $z_i$ sa $i$-ième composante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_representation =\n",
    "chunk_representation =\n",
    "\n",
    "pos_target = \n",
    "chunk_target = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement et conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consigne \n",
    "Jusqu'ici on a juste défini la succession d'opération, il s'agit maintenant de créer le modèle en utilisant la classe modèle de Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Model(inputs=,outputs=)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consigne\n",
    "- compiler le modèle (NB : il y a une loss pour chaque tâche)\n",
    "- afficher l'architecture\n",
    "- entrainer le modèle pour le nombre d'époque suffisant en validant sur X_test et Y_test et une taille de batch de 128\n",
    "- Conclure en comparant avec les performances précédentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical learning\n",
    "Une autre façon de faire du multi-task consiste à construire une architecture en cascade où les tâches n'interviennent pas à la même profondeur du réseau de neurones.\n",
    "\n",
    "### Consigne :\n",
    "- En vous inspirant de la partie précedente, entrainer un modèle en cascade de type : \n",
    "                              POS\n",
    "                            /\n",
    "EMBEDDING - DENSE - DROPOUT \n",
    "                            \\\n",
    "                              DENSE - DROPOUT - CHUNK\n",
    "                              \n",
    "Autrement dit, dans cette approche, on pré-suppose que  \n",
    "- EMBEDDING - DENSE - DROPOUT apprend une représentation suffisante pour prédire le POS-TAG\n",
    "- plus de layer sont nécessaires pour le Shallow parsing\n",
    "- la représentation intermédiaire EMBEDDING - DENSE - DROPOUT est un bon point de départ pour le shallow parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question optionnelle \n",
    "- Comment feriez vous, en utilisant la classe Model, pour pouvoir extraire l'embedding tuné sur le modèle pour un mot donné? Tenter de l'implémenter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
